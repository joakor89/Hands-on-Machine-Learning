{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c75e38",
   "metadata": {},
   "source": [
    "# Chapter 18 – Reinforcement Learning\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814347f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python ≥3.5 is required\n",
    "# import sys\n",
    "# assert sys.version_info >= (3, 5)\n",
    "\n",
    "# # Is this notebook running on Colab or Kaggle?\n",
    "# IS_COLAB = \"google.colab\" in sys.modules\n",
    "# IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# if IS_COLAB or IS_KAGGLE:\n",
    "#     !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
    "#     %pip install -U tf-agents pyvirtualdisplay\n",
    "#     %pip install -U gym~=0.21.0\n",
    "#     %pip install -U gym[box2d,atari,accept-rom-license]\n",
    "\n",
    "# # Scikit-Learn ≥0.20 is required\n",
    "# import sklearn\n",
    "# assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# # TensorFlow ≥2.0 is required\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# if not tf.config.list_physical_devices('GPU'):\n",
    "#     print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "#     if IS_COLAB:\n",
    "#         print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "#     if IS_KAGGLE:\n",
    "#         print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# # Common imports\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # to make this notebook's output stable across runs\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # To plot pretty figures\n",
    "# %matplotlib inline\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# mpl.rc('axes', labelsize=14)\n",
    "# mpl.rc('xtick', labelsize=12)\n",
    "# mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a7ed4",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8590e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb236999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym.envs.registry.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8d14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a56844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.seed(42)\n",
    "# obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7744f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cadea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyvirtualdisplay\n",
    "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828ca779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f98f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = env.render(mode=\"rgb_array\")\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0003c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_environment(env, figsize=(5,4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6463c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_environment(env)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0964be6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022fa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = 1  # accelerate right\n",
    "# obs, reward, done, info = env.step(action)\n",
    "# obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_environment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aecbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869143f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a798ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88301388",
   "metadata": {},
   "outputs": [],
   "source": [
    "if done:\n",
    "    obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb695f4",
   "metadata": {},
   "source": [
    "### A simple hard-coded policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ee3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.seed(42)\n",
    "\n",
    "# def basic_policy(obs):\n",
    "#     angle = obs[2]\n",
    "#     return 0 if angle < 0 else 1\n",
    "\n",
    "# totals = []\n",
    "# for episode in range(500):\n",
    "#     episode_rewards = 0\n",
    "#     obs = env.reset()\n",
    "#     for step in range(200):\n",
    "#         action = basic_policy(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         episode_rewards += reward\n",
    "#         if done:\n",
    "#             break\n",
    "#     totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "\n",
    "frames = []\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(200):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    action = basic_policy(obs)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1575244",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd8785",
   "metadata": {},
   "source": [
    "### Neural Network Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d155d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy_net(model, n_max_steps=200, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        left_proba = model.predict(obs.reshape(1, -1))\n",
    "        action = int(np.random.rand() > left_proba)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(model)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 50\n",
    "n_iterations = 5000\n",
    "\n",
    "envs = [gym.make(\"CartPole-v1\") for _ in range(n_environments)]\n",
    "for index, env in enumerate(envs):\n",
    "    env.seed(index)\n",
    "np.random.seed(42)\n",
    "observations = [env.reset() for env in envs]\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
    "    target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
    "                              for obs in observations])\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_probas = model(np.array(observations))\n",
    "        loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
    "    print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
    "    for env_index, env in enumerate(envs):\n",
    "        obs, reward, done, info = env.step(actions[env_index][0])\n",
    "        observations[env_index] = obs if not done else env.reset()\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(model)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401abb8",
   "metadata": {},
   "source": [
    "### Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3002ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f15d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da768f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d82e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd02926",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f039978",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(42);\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(model)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e3412",
   "metadata": {},
   "source": [
    "### Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to ...\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence():\n",
    "    current_state = 0\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03a8b8",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddfa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049ddad",
   "metadata": {},
   "source": [
    "### Q-Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90  # the discount factor\n",
    "\n",
    "history1 = [] # Not shown in the book (for the figure below)\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    history1.append(Q_prev) # Not shown\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "\n",
    "history1 = np.array(history1) # Not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dae2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd61454",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95  # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfdec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255ea50",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3743af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state][actions] = 0\n",
    "\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "state = 0 # initial state\n",
    "history2 = [] # Not shown in the book\n",
    "\n",
    "for iteration in range(10000):\n",
    "    history2.append(Q_values.copy()) # Not shown\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state]) # greedy policy at the next step\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state\n",
    "\n",
    "history2 = np.array(history2) # Not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q_values, axis=1) # optimal action for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f75ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Q_value = history1[-1, 0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].set_ylabel(\"Q-Value$(s_0, a_0)$\", fontsize=14)\n",
    "axes[0].set_title(\"Q-Value Iteration\", fontsize=14)\n",
    "axes[1].set_title(\"Q-Learning\", fontsize=14)\n",
    "for ax, width, history in zip(axes, (50, 10000), (history1, history2)):\n",
    "    ax.plot([0, width], [true_Q_value, true_Q_value], \"k--\")\n",
    "    ax.plot(np.arange(width), history[:, 0, 0], \"b-\", linewidth=2)\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax.axis([0, width, 0, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738ed6c",
   "metadata": {},
   "source": [
    "### Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b255c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c091b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72437ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88caded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5795bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24900cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = [] \n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    obs = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step) # Not shown in the book\n",
    "    if step >= best_score: # Not shown\n",
    "        best_weights = model.get_weights() # Not shown\n",
    "        best_score = step # Not shown\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ae409",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=6e-3)\n",
    "loss_fn = keras.losses.Huber()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0038fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = []\n",
    "best_score = 0\n",
    "\n",
    "for episode in range(600):\n",
    "    obs = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    if episode >= 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())\n",
    "    # Alternatively, you can do soft updates at each step:\n",
    "    #if episode >= 50:\n",
    "        #target_weights = target.get_weights()\n",
    "        #online_weights = model.get_weights()\n",
    "        #for index in range(len(target_weights)):\n",
    "        #    target_weights[index] = 0.99 * target_weights[index] + 0.01 * online_weights[index]\n",
    "        #target.set_weights(target_weights)\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77831bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(43)\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "   \n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0f390",
   "metadata": {},
   "source": [
    "### Dueling Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = keras.backend\n",
    "input_states = keras.layers.Input(shape=[4])\n",
    "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "state_values = keras.layers.Dense(1)(hidden2)\n",
    "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
    "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=7.5e-3)\n",
    "loss_fn = keras.losses.Huber()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = []\n",
    "best_score = 0\n",
    "\n",
    "for episode in range(600):\n",
    "    obs = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    if episode >= 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Sum of rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b7435",
   "metadata": {},
   "source": [
    "## Using TF-Agents to Beat Breakout\n",
    "\n",
    "### TF-Agents Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "env = suite_gym.load(\"Breakout-v4\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ddc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1) # Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.current_time_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9f491",
   "metadata": {},
   "source": [
    "### Environment Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2c777",
   "metadata": {},
   "source": [
    "### Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fda1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41366437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.environments.wrappers\n",
    "\n",
    "for name in dir(tf_agents.environments.wrappers):\n",
    "    obj = getattr(tf_agents.environments.wrappers, name)\n",
    "    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "limited_repeating_env = suite_gym.load(\n",
    "    \"Breakout-v4\",\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae20dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_repeating_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e42e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "class AtariPreprocessingWithAutoFire(AtariPreprocessing):\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        super().step(1) # FIRE to start\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        lives_before_action = self.ale.lives()\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "        if self.ale.lives() < lives_before_action and not done:\n",
    "            super().step(1) # FIRE to start after life lost\n",
    "        return obs, rewards, done, info\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessingWithAutoFire, FrameStack4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "env.reset()\n",
    "for _ in range(4):\n",
    "    time_step = env.step(3) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f82096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observation(obs):\n",
    "    # Since there are only 3 color channels, you cannot display 4 frames\n",
    "    # with one primary color per frame. So this code computes the delta between\n",
    "    # the current frame and the mean of the other frames, and it adds this delta\n",
    "    # to the red and blue channels to get a pink color for the current frame.\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plot_observation(time_step.observation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a47343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a67134",
   "metadata": {},
   "source": [
    "### Creating the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc966e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=20000) # <=> 80,000 ALE frames\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92432473",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(9) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "#trajectories, buffer_info = replay_buffer.get_next( # get_next() is deprecated\n",
    "#    sample_batch_size=2, num_steps=3)\n",
    "\n",
    "trajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n",
    "    sample_batch_size=2,\n",
    "    num_steps=3,\n",
    "    single_deterministic_pass=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54034a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6582279",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe942b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.step_type.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ccc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6.8))\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        plt.subplot(2, 3, row * 3 + col + 1)\n",
    "        plot_observation(trajectories.observation[row, col].numpy())\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd29b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2175964",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(n_iterations=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image_path = os.path.join(\"images\", \"rl\", \"breakout.gif\")\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images[0].save(image_path, format='GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all=True,\n",
    "                     duration=30,\n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"images/rl/breakout.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec844e4",
   "metadata": {},
   "source": [
    "### Extra material\n",
    "\n",
    "\n",
    "#### Deque vs Rotating List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "np.random.seed(42)\n",
    "\n",
    "mem = deque(maxlen=1000000)\n",
    "for i in range(1000000):\n",
    "    mem.append(i)\n",
    "[mem[i] for i in np.random.randint(1000000, size=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mem.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [mem[i] for i in np.random.randint(1000000, size=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = np.empty(max_size, dtype=np.object)\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(self.size, size=batch_size)\n",
    "        return self.buffer[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = ReplayMemory(max_size=1000000)\n",
    "for i in range(1000000):\n",
    "    mem.append(i)\n",
    "mem.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mem.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mem.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de8716",
   "metadata": {},
   "source": [
    "### Creating a Custom TF-Agents Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnvironment(tf_agents.environments.py_environment.PyEnvironment):\n",
    "    def __init__(self, discount=1.0):\n",
    "        super().__init__()\n",
    "        self._action_spec = tf_agents.specs.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, name=\"action\", minimum=0, maximum=3)\n",
    "        self._observation_spec = tf_agents.specs.BoundedArraySpec(\n",
    "            shape=(4, 4), dtype=np.int32, name=\"observation\", minimum=0, maximum=1)\n",
    "        self.discount = discount\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.zeros(2, dtype=np.int32)\n",
    "        obs = np.zeros((4, 4), dtype=np.int32)\n",
    "        obs[self._state[0], self._state[1]] = 1\n",
    "        return tf_agents.trajectories.time_step.restart(obs)\n",
    "\n",
    "    def _step(self, action):\n",
    "        self._state += [(-1, 0), (+1, 0), (0, -1), (0, +1)][action]\n",
    "        reward = 0\n",
    "        obs = np.zeros((4, 4), dtype=np.int32)\n",
    "        done = (self._state.min() < 0 or self._state.max() > 3)\n",
    "        if not done:\n",
    "            obs[self._state[0], self._state[1]] = 1\n",
    "        if done or np.all(self._state == np.array([3, 3])):\n",
    "            reward = -1 if done else +10\n",
    "            return tf_agents.trajectories.time_step.termination(obs, reward)\n",
    "        else:\n",
    "            return tf_agents.trajectories.time_step.transition(obs, reward,\n",
    "                                                               self.discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = MyEnvironment()\n",
    "time_step = my_env.reset()\n",
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711305bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = my_env.step(1)\n",
    "time_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
